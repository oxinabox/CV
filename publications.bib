% Encoding: windows-1252

@InProceedings{White2015TransistorPSO,
  author    = {Lyndon White and Lyndon While and Ben Deeks and Farid Boussaid},
  title     = {Transistor Sizing Using Particle Swarm Optimisation},
  booktitle = {IEEE Symposium Series on Computational Intelligence},
  year      = {2015},
  month     = {Dec},
  pages     = {259-266},
  doi       = {10.1109/SSCI.2015.46},
  abstract  = {We describe an application of particle swarm optimisation to the problem of determining the optimal sizing of transistors in an integrated circuit. The algorithm minimises the total area of silicon utilised by a given circuit, whilst maintaining the propagation delay of the circuit within a hard limit. It assesses designs using the well-known circuit simulation engine SPICE, making allowance for the inability of SPICE to assess poorly-designed circuits within a reasonable timeframe. Experiments on three different types of circuits demonstrate that the algorithm is able to derive excellent designs for a range of problem instances, including several problems where the Monte Carlo method is unable to find any feasible solutions at all.},
  keywords  = {MOSFET;Monte Carlo methods;elemental semiconductors;particle swarm optimisation;silicon;Monte Carlo method;Si;circuit simulation engine SPICE;integrated circuit;optimal transistor sizing;particle swarm optimisation;propagation delay;silicon;Algorithm design and analysis;Birds;Integrated circuit modeling;Optimization;Propagation delay;SPICE;Transistors},
  owner     = {20361362},
  timestamp = {2017.05.03},
}

@InProceedings{White2015SentVecMeaning,
  author    = {White, Lyndon and Togneri, Roberto and Liu, Wei and Bennamoun, Mohammed},
  title     = {How Well Sentence Embeddings Capture Meaning},
  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium},
  year      = {2015},
  series    = {ADCS '15},
  publisher = {ACM},
  location  = {Parramatta, NSW, Australia},
  isbn      = {978-1-4503-4040-3},
  pages     = {9:1--9:8},
  doi       = {10.1145/2838931.2838932},
  url       = {http://doi.acm.org/10.1145/2838931.2838932},
  abstract  = {Several approaches for embedding a sentence into a vector space have been developed. However, it is unclear to what extent the sentence's position in the vector space reflects its semantic meaning, rather than other factors such as syntactic structure. Depending on the model used for the embeddings this will vary -- different models are suited for different down-stream applications. For applications such as machine translation and automated summarization, it is highly desirable to have semantic meaning encoded in the embedding. We consider this to be the quality of semantic localization for the model -- how well the sentences' meanings coincides with their embedding's position in vector space. Currently the semantic localization is assessed indirectly through practical benchmarks for specific applications.  In this paper, we ground the semantic localization problem through a semantic classification task. The task is to classify sentences according to their meaning. A SVM with a linear kernel is used to perform the classification using the sentence vectors as its input. The sentences from subsets of two corpora, the Microsoft Research Paraphrase corpus and the Opinosis corpus, were partitioned according to their semantic equivalence. These partitions give the target classes for the classification task. Several existing models, including URAE, PV--DM and PV--DBOW, were assessed against a bag of words benchmark},
  acmid     = {2838932},
  articleno = {9},
  keywords  = {Semantic vector space representations, semantic consistency evaluation, sentence embeddings, word embeddings},
  numpages  = {8},
  owner     = {20361362},
  timestamp = {2015.11.19},
}

@InProceedings{White2016BOWgen,
  author    = {White, Lyndon and Togneri,Roberto and Liu,Wei and Bennamoun, Mohammed},
  title     = {Generating Bags of Words from the Sums of their Word Embeddings},
  booktitle = {17th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing)},
  year      = {2016},
  abstract  = {Many methods have been proposed to generate sentence vector representations, such as recursive neural networks, latent distributed memory models, and the simple sum of word embeddings (SOWE). However, very few methods demonstrate the ability to reverse the process -- recovering sentences from sentence embeddings. Amongst the many sentence embeddings, SOWE has been shown to maintain semantic meaning, so in this paper we introduce a method for moving from the SOWE representations back to the bag of words (BOW) for the original sentences. This is a part way step towards recovering the whole sentence and has useful theoretical and practical applications of its own. This is done using a greedy algorithm to convert the vector to a bag of words. To our knowledge this is the first such work. It demonstrates qualitatively the ability to recreate the words from a large corpus based on its sentence embeddings. As well as practical applications for allowing classical information retrieval methods to be combined with more recent methods using the sums of word embeddings, the success of this method has theoretical implications on the degree of information maintained by the sum of embeddings representation. This lends some credence to the consideration of the SOWE as a dimensionality reduced, and meaning enhanced, data manifold for the bag of words. },
  timestamp = {2013.08.15},
  note = {Best Student Paper Award},
}

@InProceedings{White2016SOWE2Sent,
  author    = {White, Lyndon and Togneri,Roberto and Liu,Wei and Bennamoun, Mohammed},
  title     = {Modelling Sentence Generation from Sum of Word Embedding Vectors as a Mixed Integer Programming Problem},
  booktitle = {IEEE International Conference on Data Mining: High Dimensional Data Mining Workshop (ICDM: HDM)},
  year      = {2016},
  doi       = {10.1109/ICDMW.2016.0113},
  url       = {http://white.ucc.asn.au/publications/White2016SOWE2Sent.pdf},
  abstract  = {Converting a sentence to a meaningful vector representation has uses in many NLP tasks, however very few methods allow that representation to be restored to a human readable sentence. Being able to generate sentences from the vector representations demonstrates the level of information maintained by the embedding representation – in this case a simple sum of word embeddings. We introduce such a method for moving from this vector representation back to the original sentences. This is done using a two stage process; first a greedy algorithm is utilised to convert the vector to a bag of words, and second a simple probabilistic language model is used to order the words to get back the sentence. To the best of our knowledge this is the first work to demonstrate quantitatively the ability to reproduce text from a large corpus based directly on its sentence embeddings},
  owner     = {20361362},
  timestamp = {2017.02.28},
}

@Article{White2018RefittingSenses,
  author   = {White, Lyndon and Togneri,Roberto and Liu,Wei and Bennamoun, Mohammed},
  title    = {Finding Word Sense Embeddings Of Known Meaning},
  journal  = {19th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing)},
  year     = {2018},
  abstract = {Word sense embeddings are vector representations of polysemous words -- words with multiple meanings. These induced sense embeddings, however, do not necessarily correspond to any dictionary senses of the word. This limits their applicability in traditional semantic-orientated tasks such as lexical word sense disambiguation. To overcome this, we propose a method to find new sense embeddings of known meaning. We term this method refitting, as the new embedding is fitted to model the meaning of a target word in the example sentence. This is accomplished using the probabilities of the existing induced sense embeddings, as well as their vector values. Our contributions are threefold: (1) The refitting method to find the new sense embeddings;  (2) a novel smoothing technique, for use with the refitting method; and (3) a new similarity measure for words in context, defined by using the refitted sense embeddings. We show how our techniques improve the performance of the Adaptive Skip-Gram sense embeddings for word similarly evaluation; and how they allow the embeddings to be used for lexical word sense disambiguation -- which was not possible using the induced sense embeddings.},
}

@InProceedings{Sharif2018LearningCompositeMetrics,
  author    = {Sharif, Naeha and White, Lyndon and and Bennamoun, Mohammed and Shah, Syed Afaq Ali},
  title     = {Learning-based Composite Metrics for Improved Caption Evaluation},
  booktitle = {Proceedings of the ACL Student Research Workshop},
  year      = {2018},
  publisher = {Association for Computational Linguistics},
  url       = {https://white.ucc.asn.au/publications/Sharif2018LearningCompositeMetrics.pdf},
  abstract  = {The evaluation of image caption quality is a challenging task, which requires the assessment of two main aspects in a caption: adequacy and fluency. These quality aspects can be judged using a combination of several linguistic features. However, most of the current image captioning metrics focus only on specific linguistic facets, such as the lexical or semantic, and fail to meet a satisfactory level of correlation with human judgements at the sentence-level. We propose a learning-based framework to incorporate the scores of a set of lexical and semantic metrics as features, to capture the adequacy and fluency of captions at different linguistic levels. Our experimental results demonstrate that composite metrics draw upon the strengths of standalone measures to yield improved correlation and accuracy.}
}


@InProceedings{White2018NovelPerspective,
  author    = {White, Lyndon and Togneri,Roberto and Liu,Wei and Bennamoun, Mohammed},
  title     = {NovelPerspective: Identifying Point of View Characters},
  booktitle = {Proceedings of ACL 2018, System Demonstrations},
  year      = {2018},
  publisher = {Association for Computational Linguistics},
  abstract  = {We present NovelPerspective: a tool to allow consumers to subset their digital literature, based on point of view (POV) character. Many novels have multiple main characters each with their own storyline running in parallel. A well-known example is George R. R. Martin's  novel: ``A Game of Thrones'', and others from that series. Our tool detects the main character that each section is from the POV of, and allows the user to generate a new ebook with only those sections. This gives consumers new options in how they consume their media; allowing them to  pursue the storylines sequentially, or skip chapters about characters they find boring. We present two heuristic-based baselines, and two machine learning based methods for the detection of the main character.},
  owner     = {20361362},
  timestamp = {2018.06.15},
}

@Book{white2018NRoNL,
  author    = {White, Lyndon and Togneri,Roberto and Liu,Wei and Bennamoun, Mohammed},
  title     = {Neural Representations of Natural Language},
  year      = {2018},
  series    = {Studies in Computational Intelligence (Book)},
  publisher = {Springer Singapore},
  isbn      = {9789811300615},
  url       = {https://www.springer.com/us/book/9789811300615},
  owner     = {20361362},
  timestamp = {2018.06.15},
}

@InProceedings{Sharif2018NNEval,
  author    = {Sharif, Naeha and White, Lyndon and Bennamoun, Mohammed and Shah, Syed Afaq Ali},
  title     = {NNEval: Neural Network based Evaluation Metric},
  booktitle = {Proceedings of the 15th European Conference on Computer Vision},
  year      = {2018},
  publisher = {Springer Lecture Notes in Computer Science},
  url       = {https://white.ucc.asn.au/publications/Sharif2018NNEval.pdf},
  owner     = {20361362},
  timestamp = {2018.07.16},
}

@article{malmaud2018tensorflow,
  title={TensorFlow. jl: An idiomatic Julia front end for TensorFlow},
  author={Malmaud, Jonathan and White, Lyndon},
  journal={Journal of Open Source Software},
  volume={3},
  number={31},
  pages={1002},
  year={2018}
}


@phdthesis{05c72e2cd52e4321b1414c897e33a174,
  title = "On the surprising capacity of linear combinations of embeddings for natural language processing",
  abstract = "Natural language is the human expression of thought. The representation of natural language, and thus thought, is fundamental to the field of artificial intelligence. This thesis explores that representation through weighted sums of embeddings. Embeddings are dense numerical vectors representing natural language components (e.g. words). Their sum is commonly overlooked as being too simple: in contrast to sequence or tree representations. However, we find that on numerous real-world problems it is actually superior. This thesis demonstrates this capacity, and explains why. The sum of embeddings is a particularly effective dimensionality-reduced representation of the crucial surface features of language.",
  keywords = "embeddings, Natural language processing, Machine Learning, natural language understanding, artificial intelligence, sum of word embeddings, Neural Networks, computer science",
  author = "Lyndon White",
  year = "2019",
  doi = "10.26182/w0c2-6887",
  language = "English",
  school = "The University of Western Australia",
}


@article{e81a4a803be14fe692d174da0dfd1864,
  title = "LCEval: Learned Composite Metric for Caption Evaluation",
  abstract = "Automatic evaluation metrics hold a fundamental importance in the development and fine-grained analysis of captioning systems. While current evaluation metrics tend to achieve an acceptable correlation with human judgements at the system level, they fail to do so at the caption level. In this work, we propose a neural network-based learned metric to improve the caption-level caption evaluation. To get a deeper insight into the parameters which impact a learned metric{\textquoteright}s performance, this paper investigates the relationship between different linguistic features and the caption-level correlation of the learned metrics. We also compare metrics trained with different training examples to measure the variations in their evaluation. Moreover, we perform a robustness analysis, which highlights the sensitivity of learned and handcrafted metrics to various sentence perturbations. Our empirical analysis shows that our proposed metric not only outperforms the existing metrics in terms of caption-level correlation but it also shows a strong system-level correlation against human assessments.",
  keywords = "Accuracy, Automatic evaluation metric, Correlation, Image captioning, Learned metrics, Neural networks, Robustness",
  author = "Naeha Sharif and Lyndon White and Mohammed Bennamoun and Wei Liu and Shah, {Syed Afaq Ali}",
  year = "2019",
  month = oct,
  day = "1",
  doi = "10.1007/s11263-019-01206-z",
  language = "English",
  volume = "127",
  pages = "1586--1610",
  journal = "International Journal of Computer Vision",
  issn = "0920-5691",
  publisher = "Springer",
  number = "10",
}

@inproceedings{jamei2019meta,
  title={Meta-optimization of optimal power flow},
  author={Jamei, Mahdi and Mones, Letif and Robson, Alex and White, Lyndon and Requeima, James and Ududec, Cozmin},
  booktitle={ICML Workshop, Climate Change: How Can AI Help},
  year={2019}
}

@article{Kaushal2020,
 doi = {10.21105/joss.01956},
 url = {https://doi.org/10.21105/joss.01956},
 year = {2020},
 publisher = {The Open Journal},
 volume = {5},
 number = {46},
 pages = {1956},
 author = {Ayush Kaushal and Lyndon White and Mike Innes and Rohit Kumar},
 title = {WordTokenizers.jl: Basic tools for tokenizing natural language in Julia},
 journal = {Journal of Open Source Software},
} 


@inproceedings{45d6b4c011df45fb9fd188d66e14cbfe,
  title = "WEmbSim: A Simple yet Effective Metric for Image Captioning",
  abstract = "The area of automatic image caption evaluation is still undergoing intensive research to address the needs of generating captions which can meet adequacy and fluency requirements. Based on our past attempts at developing highly sophisticated learning-based metrics, we have discovered that a simple cosine similarity measure using the Mean of Word Embeddings (MOWE) of captions can actually achieve a surprisingly high performance on unsupervised caption evaluation. This inspires our proposed work on an effective metric WEmbSim, which beats complex measures such as SPICE, CIDEr and WMD at system-level correlation with human judgments. Moreover, it alsoachieves the best accuracy at matching human consensus scores for caption pairs, against commonly used unsupervised methods. Therefore, we believe that WEmbSim sets a new baseline for any complex metric to be justified.",
  author = "Naeha Sharif and Lyndon White and Mohammed Bennamoun and Wei Liu and Shah, {Syed Afaq Ali}",
  year = "2020",
  month = nov,
  day = "29",
  language = "English",
  publisher = "IEEE, Institute of Electrical and Electronics Engineers",
  booktitle = "Digital Image Computing: Techniques and Applications, 2020 (DICTA 2020)",
  address = "United States",
  note = "DSTG Best Contribution to Science Award"
}

@inproceedings{https://doi.org/10.48550/arxiv.2109.12449,
  doi = {10.48550/ARXIV.2109.12449},
  url = {https://arxiv.org/abs/2109.12449},
  author = {Schäfer, Frank and Tarek, Mohamed and White, Lyndon and Rackauckas, Chris},
  keywords = {Mathematical Software (cs.MS), Machine Learning (cs.LG), Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {AbstractDifferentiation.jl: Backend-Agnostic Differentiable Programming in Julia},
  booktitle = {NeurIPS Differentiable Programming Workshop},
  year = {2021},
  note = {Best Poster Award},
}